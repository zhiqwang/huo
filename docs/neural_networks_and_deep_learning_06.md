---
layout: post
title: Deep learning
tags: [deep-learning, universality]
date: 2018-05-18 19:00:00 +0800
---

### Introducing convolutional networks
- But upon reflection, it's strange to use networks with fully-connected layers to classify images. The reason is that such a network architecture does not take into account the spatial structure of the images. For instance, it treats input pixels which are far apart and close together on exactly the same footing. Such concepts of spatial structure must instead be inferred from the training data. But what if, instead of starting with a network architecture which is _tabula rasa_, we used an architecture which tries to take advantage of the spatial structure? The _convolutional neural networks_ use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, many-layer networks, which are very good at classifying images.
- **Local receptive fields:** As per usual, we'll connect the input pixels to a layer of hidden neurons. But we won't connect every input pixel to every hidden neuron. Instead, we only make connections in small, localized regions of the input image. That region in the input image is called the _local receptive field_ for the hidden neuron. It's a little window on the input pixels. Each connection learns a weight. And the hidden neuron learns an overall bias as well. You can think of that particular hidden neuron as learning to analyze its particular local receptive field.
- **Shared weights and biases:** I've said that each hidden neuron has a bias and $5 \times 5$ weights connected to its its local receptive field. What I did not yet mention is that we're going to use the _same_ weights and bias for each of the $24 \times 24$ hidden neurons. This means that all the neurons in the first hidden layer detect exactly the same feature, just at different locations in the input image. We sometimes call the map from the input layer to the hidden layer a _feature map._ We call the weights defining the feature map the _shared weights._ And we call the bias defining the feature map in this way the _shared bias._ The shared weights and bias are often said to define a _kernel_ or _filter._
- **Pooling layers:** _Pooling layers_ are usually used immediately after convolutional layers. What the pooling layers do is simplify the information in the output from the convolutional layer. In detail, a pooling layer takes each feature map output from the convolutional layer and prepares a condensed feature map. We can think of max-pooling as a way for the network to ask whether a given feature is found anywhere in a region of the image. It then throws away the exact positional information. The intuition is that once a feature has been found, its exact location isn't as important as its rough location relative to other features. A big benefit is that there are many fewer pooled features, and so this helps reduce the number of parameters needed in later layers.

### Convolutional neural networks in practice
- In this architecture, we can think of the convolutional and pooling layers as learning about local spatial structure in the input training image, while the later, fully-connected layer learns at a more abstract level, integrating global information from across the entire image. This is a common pattern in convolutional neural networks.
- The first question is: what does it even mean to apply a second convolutional-pooling layer? In fact, you can think of the second convolutional-pooling layer as having as input $12 \times 12$ "images", whose "pixels" represent the presence (or obsence) of particular localized features in the original input image. So you can think of this layer as having as input a version of the original input image. That version is abstracted and condensed, but still has a lot of spatial structure, and so it makes sense to use a second convolutional-pooling layer.
- That'a a satisfying point of view, but gives rise to a second question. The output from the previous layer involves 20 separate feature maps, and so there are $20 \times 12 \times 12$ inputs to the second convolutional-pooling layer. It's as though we've got $20$ separate images input to the convolutional-pooling layer, not a single image, as was the case for the first convolutional-pooling layer. How should neurons in the second convolutional-pooling layer respond to these multiple input images? In fact, we'll allow each neuron in this layer to learn from _all_ $20 \times 5 \times 5$ input neurons in its local receptive field. More informally: the feature detectors in the second convolutional-pooling layer have access to _all_ the features from the previous layer, but only within their particular local receptive field.
- Across all my experiments I found that networks based on rectified linear units consistently outperformed networks based on sigmoid activation functions. There appears to be a real gain in moving to rectified linear units for this problem. What makes the rectified linear activation function better than the sigmoid or tanh functions? Indeed, rectified linear units have only begun to be widely used in the past few years. The reason for that recent adoption is empirical: a few people tried rectified linear units, often on the basis of hunches or heuristic arguments. They got good results classifying classifying benchmark data sets, and the practice has spread.
- **Why we only appled dropout to the fully-connected layers:** The convolutional layers have considerable inbuilt resistance to overfitting. The reason is that the shared weights means that convolutional filters are foced to learn from across the entire image. This makes them less likely to pick up on local idiosyncracies in the training data. And so there is less need to apply other regularizers, such as dropout.
